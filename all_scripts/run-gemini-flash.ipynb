{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7a02fb-fe2e-47aa-b6ac-f4d4b755595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, re\n",
    "from google import genai\n",
    "from prompts import *\n",
    "\n",
    "client = genai.Client(api_key=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b921b56-c403-4632-a646-7491d1a30a04",
   "metadata": {},
   "source": [
    "#### Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42efedea-2b6d-4af3-9d37-6a055cfe7b93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 100\n",
      "Count 200\n",
      "Count 300\n",
      "Count 400\n",
      "Count 500\n",
      "Count 600\n",
      "Count 700\n",
      "Count 800\n",
      "Count 900\n",
      "Count 1000\n",
      "Count 1100\n",
      "Count 1200\n",
      "Count 1300\n",
      "Count 1400\n",
      "Count 1500\n",
      "Count 1600\n",
      "Count 1700\n",
      "Count 1800\n",
      "Count 1900\n",
      "Count 2000\n",
      "Count 2100\n",
      "Count 2200\n",
      "Count 2300\n",
      "Count 2400\n",
      "Count 2500\n",
      "Count 2600\n",
      "Count 2700\n",
      "Count 2800\n",
      "Count 2900\n",
      "Count 3000\n",
      "Count 3100\n",
      "Count 3200\n",
      "Count 3300\n",
      "Count 3400\n",
      "Count 3500\n",
      "Count 3600\n",
      "Count 3700\n",
      "Count 3800\n",
      "Count 3900\n",
      "Count 4000\n",
      "Count 4100\n",
      "Count 4200\n",
      "Count 4300\n",
      "Count 4400\n",
      "Count 4500\n",
      "Count 4600\n",
      "Count 4700\n",
      "Count 4800\n",
      "Count 4900\n",
      "Count 5000\n",
      "Processing complete. Results written to /speed-scratch/ra_mdash/results/prime_vul/gemini-2.0-flash_zero_shot.jsonl\n"
     ]
    }
   ],
   "source": [
    "def run_on_data(data_path, output_file):\n",
    "    count = 0\n",
    "    max_test = 5000\n",
    "    \n",
    "    # start = 396\n",
    "    # count = start\n",
    "    # end = 405\n",
    "    debug = False #True #False\n",
    "\n",
    "    if prompting == \"few_shot\":\n",
    "        prompt_template = get_few_shot_prompt()\n",
    "    elif prompting == \"zero_shot\":\n",
    "        prompt_template = get_zero_shot_prompt()    \n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    with open(data_path, \"r\") as f, open(output_file, \"a\") as output_f:\n",
    "        for line in f:\n",
    "            # Clear unused GPU memory and collect garbage\n",
    "            # torch.cuda.empty_cache()\n",
    "            # gc.collect()\n",
    "            \n",
    "            # if count >= start and count <= end:\n",
    "            if count < max_test:\n",
    "                if line.strip():\n",
    "                    try:\n",
    "                        data = json.loads(line)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error decoding JSON: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract fields from the JSON line\n",
    "                    project = data.get(\"project\", \"\")\n",
    "                    commit_message = data.get(\"commit_message\", \"\")\n",
    "                    func = data.get(\"func\", \"\")\n",
    "                    target = data.get(\"target\", \"\")\n",
    "                    cwe = data.get(\"cwe\", [])\n",
    "                    cve = data.get(\"cve\", \"\")\n",
    "                    cve_desc = data.get(\"cve_desc\", \"\")\n",
    "    \n",
    "                    final_prompt = prompt_template.format(\n",
    "                        commit_message=commit_message, func=func\n",
    "                    )\n",
    "                    # print(final_prompt)\n",
    "    \n",
    "                    response = client.models.generate_content(\n",
    "                        model=model_name, contents=final_prompt\n",
    "                    )\n",
    "\n",
    "                    match = re.match(r'^(Yes|No)\\b[.]*\\s*(.*)', response.text, re.DOTALL)\n",
    "                    \n",
    "                    if match:\n",
    "                        result = match.group(1)  # Captures 'Yes' or 'No'\n",
    "                        cot = match.group(2).strip()\n",
    "                    else:\n",
    "                        result = -1\n",
    "                        cot = response.text\n",
    "\n",
    "    \n",
    "                    output_data = {\n",
    "                        \"project\": project,\n",
    "                        \"commit_message\": commit_message,\n",
    "                        \"func\": func,\n",
    "                        \"target\": target,\n",
    "                        \"cwe\": cwe,\n",
    "                        \"cve\": cve,\n",
    "                        \"cve_desc\": cve_desc,\n",
    "                        \"result\": result,\n",
    "                        \"cot\": cot\n",
    "                    }\n",
    "    \n",
    "                    output_f.write(json.dumps(output_data) + \"\\n\")\n",
    "                    \n",
    "                    # Pause briefly and increment count\n",
    "                    time.sleep(1)\n",
    "                    count += 1\n",
    "                    if count % 100 == 0:\n",
    "                        print(\"Count\", count)\n",
    "    \n",
    "    print(f\"Processing complete. Results written to {output_file}\")\n",
    "    \n",
    "        \n",
    "# def main(data_path, model_id, prompting, output_file):\n",
    "#     if prompting == \"few_shot\":\n",
    "#         prompt_template = get_few_shot_prompt()\n",
    "#     elif prompting == \"zero_shot\":\n",
    "#         prompt_template = get_zero_shot_prompt()\n",
    "    \n",
    "#     generator = initiate_model(model_id)\n",
    "#     run_on_data(data_path, prompt_template, generator, output_file)\n",
    "    \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "    # parser = argparse.ArgumentParser(description=\"Run PrimeVul processing with specified parameters.\")\n",
    "    \n",
    "    # parser.add_argument(\"--data_path\", type=str,\n",
    "    #                     help=\"Path to the input JSONL data file.\")\n",
    "    # parser.add_argument(\"--model_id\", type=str, default=\"bigcode/starcoder2-3b\",\n",
    "    #                     help=\"Identifier of the model to be used (e.g., bigcode/starcoder2-3b).\")\n",
    "    # parser.add_argument(\"--prompting\", type=str, default=\"zero_shot\",  # few_shot, zero_shot\n",
    "    #                     help=\"Type of prompting to use (default: zero_shot).\")\n",
    "    # parser.add_argument(\"--output_file\", type=str, default=None,\n",
    "    #                     help=\"Path to the output file. If not provided, a default path is generated.\")\n",
    "    \n",
    "# args = parser.parse_args()\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "prompting = \"zero_shot\"\n",
    "\n",
    "# Set default paths if not provided\n",
    "\n",
    "data_path = \"/speed-scratch/ra_mdash/PrimeVul_v0.1/primevul_valid.jsonl\" #primevul_valid sample_data\n",
    "\n",
    "output_file = f\"/speed-scratch/ra_mdash/results/prime_vul/{model_name}_{prompting}.jsonl\"\n",
    "\n",
    "run_on_data(data_path, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d33fe-906e-4a58-9386-e6baa5081c45",
   "metadata": {},
   "source": [
    "#### Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cba1ad-e026-4505-bfdb-88d09668f09e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 100\n"
     ]
    }
   ],
   "source": [
    "def run_on_data(data_path, output_file):\n",
    "    count = 0\n",
    "    max_test = 5000\n",
    "    \n",
    "    # start = 396\n",
    "    # count = start\n",
    "    # end = 405\n",
    "    debug = False #True #False\n",
    "\n",
    "    # if prompting == \"few_shot\":\n",
    "    #     prompt_template = get_zero_shot_prompt()\n",
    "    # elif prompting == \"zero_shot\":\n",
    "    prompt_template = get_zero_shot_prompt()    \n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(data_path, \"r\") as f:\n",
    "        samples = json.load(f)\n",
    "    \n",
    "    with open(output_file, \"a\") as output_f:\n",
    "        for line in samples:\n",
    "            # Clear unused GPU memory and collect garbage\n",
    "            # torch.cuda.empty_cache()\n",
    "            # gc.collect()\n",
    "            \n",
    "            # if count >= start and count <= end:\n",
    "            if count < max_test:\n",
    "                data = line\n",
    "                # if line.strip():\n",
    "                #     try:\n",
    "                #         data = json.loads(line)\n",
    "                #     except json.JSONDecodeError as e:\n",
    "                #         print(f\"Error decoding JSON: {e}\")\n",
    "                #         continue\n",
    "                    \n",
    "                # Extract fields from the JSON line\n",
    "                project = data.get(\"project\", \"\")\n",
    "                commit_message = data.get(\"commit_message\", \"\")\n",
    "                func = data.get(\"func\", \"\")\n",
    "                target = data.get(\"target\", \"\")\n",
    "                cwe = data.get(\"cwe\", [])\n",
    "                cve = data.get(\"cve\", \"\")\n",
    "                cve_desc = data.get(\"cve_desc\", \"\")\n",
    "                fallback = data.get(\"fallback\", \"\")\n",
    "                # reasoning = data.get(\"reasoning\", \"\")\n",
    "\n",
    "                few_shot_examples = data.get(\"few_shot_samples\", \"\")\n",
    "                zero_shot_query = prompt_template.format(\n",
    "                    commit_message=commit_message,\n",
    "                    func=func\n",
    "                )\n",
    "                # print(\"zero_shot_query\")\n",
    "                # print(zero_shot_query)\n",
    "                # Concatenate the few-shot examples with the zero-shot prompt.\n",
    "                # Few-shot examples should provide context for the LLM prior to the query.\n",
    "                final_prompt = f\"{few_shot_examples}\\n\\n{zero_shot_query}\"\n",
    "\n",
    "                # print(final_prompt)\n",
    "\n",
    "                response = client.models.generate_content(\n",
    "                    model=model_name, contents=final_prompt\n",
    "                )\n",
    "                # print(response.text)\n",
    "                # print(\"\\n\\n\")\n",
    "\n",
    "                match = re.match(r'(?:Answer:\\s*)?(Yes|No)\\b[.]*\\s*(.*)', response.text, re.DOTALL)\n",
    "                \n",
    "                if match:\n",
    "                    result = match.group(1)  # Captures 'Yes' or 'No'\n",
    "                    cot = match.group(2).strip()\n",
    "                else:\n",
    "                    result = -1\n",
    "                    cot = response.text\n",
    "\n",
    "\n",
    "                output_data = {\n",
    "                    \"project\": project,\n",
    "                    \"commit_message\": commit_message,\n",
    "                    \"func\": func,\n",
    "                    \"target\": target,\n",
    "                    \"cwe\": cwe,\n",
    "                    \"cve\": cve,\n",
    "                    \"cve_desc\": cve_desc,\n",
    "                    \"result\": result,\n",
    "                    \"cot\": cot,\n",
    "                    \"fallback\": fallback,\n",
    "                    # \"reasoning\": reasoning\n",
    "                }\n",
    "\n",
    "                output_f.write(json.dumps(output_data) + \"\\n\")\n",
    "                \n",
    "                # Pause briefly and increment count\n",
    "                time.sleep(1)\n",
    "                count += 1\n",
    "                if count % 100 == 0:\n",
    "                    print(\"Count\", count)\n",
    "    \n",
    "    print(f\"Processing complete. Results written to {output_file}\")\n",
    "    \n",
    "# args = parser.parse_args()\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "prompting = \"few_shot\"\n",
    "\n",
    "# Set default paths if not provided\n",
    "\n",
    "data_path = \"/speed-scratch/ra_mdash/PrimeVul_v0.1/primevul_with_4_shot.json\"\n",
    "\n",
    "output_file = f\"/speed-scratch/ra_mdash/results/prime_vul/{model_name}_{prompting}.jsonl\"\n",
    "\n",
    "run_on_data(data_path, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be89f7-269d-4b98-9621-c5ba87d8e49c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
